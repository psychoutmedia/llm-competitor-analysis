# ğŸ¯ LLMâ€¯Competitorâ€¯Analysis

![Tests](https://img.shields.io/badge/tests-passing-brightgreen)
![BudgetÂ Guard](https://img.shields.io/badge/%F0%9F%92%B8-cost%20logged-blue)


**ElevatorÂ Pitch â€” for Everyday Powerâ€‘Users**  
> *Stop guessing which AI model best fits your workflow: this script tests six leading LLMs on any topic you pick, ranks their answers instantly, and shows exactly what each run costsâ€”so you can choose the smartest model for the lowest spend.*

Main Aim - Run a single Python command to pit todayâ€™s top LLM clouds against each other with a complex and nuanced question, autoâ€‘log cost, and get an unbiased JSON leaderboard â€” clean architecture, defensive coding, and multiâ€‘vendor expertise in under 30â€¯seconds.

---

## âš¡ Skills demonstrated

| SkillÂ Area | Evidence in This Repo |
|------------|----------------------|
| **Productionâ€‘ready Python** | Typed modules, `logging`, `dotenv`, `preâ€‘commit`, Black/Ruffâ€‘formatted. |
| **Costâ€‘aware AI engineering** | `openai_guard.py` logs tokens â†’ CSV, enforces monthly Â£ cap, modelâ€‘specific shims. |
| **Multiâ€‘cloud mastery** | Helpers for OpenAI, Anthropic, GoogleÂ Gemini, DeepSeek, Groq (Llamaâ€‘3), and local Ollama. |
| **Resilience** | Regex fallback + deterministic retry if judge JSON is malformed. |
| **Clean API design** | Provider registry â†’ dropâ€‘in new vendors in one line. |
| **Testing culture** | Pytest mocks guard cost & judge parse (green badge above). |
| **DevÂ UX polish** | Oneâ€‘liner QuickÂ Start, `.env.example`, animated GIF demo. |

Pragmatic tradeâ€‘offs: tokenâ€‘parity fairness, graceful degradation, and budget shields that translate 1â€‘toâ€‘1 into enterprise reliability.

---

## ğŸ—ºï¸ Architecture Diagram

```mermaid
graph TB
    subgraph CostÂ Guard
        A[gpt_call()] --> CSV[usage_log.csv]
    end
    Q[PromptÂ Generator (gptâ€‘4oâ€‘mini)] -->|question| B[ProviderÂ Fanâ€‘out]
    B --> OpenAI[gptâ€‘4oâ€‘mini]
    B --> Claude[claudeâ€‘3â€‘sonnet]
    B --> Gemini[geminiâ€‘flash]
    B --> DeepSeek
    B --> Groq[Llamaâ€‘3]
    B --> Ollama[local Llamaâ€‘3]
    OpenAI -->|answers| C[AnswerÂ Collector]
    Claude --> C
    Gemini --> C
    DeepSeek --> C
    Groq --> C
    Ollama --> C
    C --> JudgePrompt
    JudgePrompt --> Judge[o3â€‘mini]
    Judge -->|JSON ranks| Leaderboard
    A -. logs .-> Leaderboard
```

---

## ğŸš€ QuickÂ Start

```bash
# 1Â Clone & enter
$ git clone https://github.com/yourâ€‘user/llmâ€‘competitorâ€‘analysis.git && cd llmâ€‘competitorâ€‘analysis

# 2Â Create env & install
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# 3Â Add keys (see .env.example for all options)
$ cp .env.example .env && $EDITOR .env  # paste your secrets

# 4Â Run the showdown
$ python llm_competitor_analysis.py
```

Youâ€™ll get console banners for each vendorâ€™s answer, followed by a tidy leaderboard and a `usage_log.csv` showing you spent pennies.

---

## ğŸ› ï¸ Tech Stack

- **PythonÂ 3.11**  â€¢  **OpenAIÂ Python SDKÂ â‰¥Â 1.23**  â€¢  **AnthropicÂ SDK**  â€¢  `pythonâ€‘dotenv`  â€¢  **Black/Ruff**  â€¢  **pytest**

---

## ğŸ” What to LookÂ for in the Code

* `openai_guard.py`Â â€” modelâ€‘aware param shims, FX conversion, CSV logger.
* `providers.py`Â â€” featherâ€‘weight wrappers with identical signatures.
* `llm_competitor_analysis.py`Â â€” main flow, retry logic, regex JSON rescue.
* `tests/`Â â€” unit tests mocking provider responses and costâ€‘guard maths.

Each commit message follows ConventionalÂ Commits for clean history.



## ğŸ“„ License

Released under the MIT License â€” fork, iterate, and impress.
