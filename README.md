# ğŸ¯ LLMâ€¯Competitorâ€¯Analysis

![Tests](https://img.shields.io/badge/tests-passing-brightgreen)
![BudgetÂ Guard](https://img.shields.io/badge/%F0%9F%92%B8-cost%20logged-blue)


**ElevatorÂ Pitch â€” for Everyday Powerâ€‘Users**  
> *Stop guessing which AI model best fits your workflow: this script tests six leading LLMs on any topic you pick, ranks their answers instantly, and shows exactly what each run costsâ€”so you can choose the smartest model for the lowest spend.*

Main Aim - Run a single Python command to pit todayâ€™s top LLM clouds against each other with a complex and nuanced question, autoâ€‘log cost, and get an unbiased JSON leaderboard â€” clean architecture, defensive coding, and multiâ€‘vendor expertise in under 30â€¯seconds.

---

## âš¡ Skills demonstrated

| SkillÂ Area | Evidence in This Repo |
|------------|----------------------|
| **Productionâ€‘ready Python** | Typed modules, `logging`, `dotenv`, `preâ€‘commit`, Black/Ruffâ€‘formatted. |
| **Costâ€‘aware AI engineering** | `openai_guard.py` logs tokens â†’ CSV, enforces monthly Â£ cap, modelâ€‘specific shims. |
| **Multiâ€‘cloud mastery** | Helpers for OpenAI, Anthropic, GoogleÂ Gemini, DeepSeek, Groq (Llamaâ€‘3), and local Ollama. |
| **Resilience** | Regex fallback + deterministic retry if judge JSON is malformed. |
| **Clean API design** | Provider registry â†’ dropâ€‘in new vendors in one line. |
| **Testing culture** | Pytest mocks guard cost & judge parse (green badge above). |
| **DevÂ UX polish** | Oneâ€‘liner QuickÂ Start, `.env.example`, animated GIF demo. |

Pragmatic tradeâ€‘offs: tokenâ€‘parity fairness, graceful degradation, and budget shields that translate 1â€‘toâ€‘1 into enterprise reliability.

---

## ğŸ—ºï¸ Architecture Diagram

%% LLMâ€‘Competitorâ€‘Analysis â€” highâ€‘level flow
flowchart LR
    subgraph CostGuard[CostÂ Guard ğŸ”’]
        style CostGuard stroke-dasharray: 4 4
        A[gpt_call()] --> CSV[usage_log.csv]
    end

    Q["PromptÂ Gen<br/>(gptâ€‘4oâ€‘mini)"] -->|question| Fan[ProviderÂ Fanâ€‘out]

    %% Competitor calls
    Fan --> OA[gptâ€‘4oâ€‘mini]
    Fan --> CL[claudeâ€‘3]
    Fan --> GM[geminiâ€‘flash]
    Fan --> DS[deepseek]
    Fan --> GR[llamaâ€‘3Â (Groq)]
    Fan --> OL[llamaâ€‘3Â (Ollama)]

    %% Collect answers
    OA --> Coll[AnswerÂ Collector]
    CL --> Coll
    GM --> Coll
    DS --> Coll
    GR --> Coll
    OL --> Coll

    %% Judge path
    Coll --> JP[JudgeÂ Prompt]
    JP --> J[o3â€‘miniÂ judge]
    J -->|JSON rank| LB[Leaderboard]

    %% Log arrow
    A -. logs .-> LB
---

## ğŸš€ QuickÂ Start

```bash
# 1Â Clone & enter
$ git clone https://github.com/yourâ€‘user/llmâ€‘competitorâ€‘analysis.git && cd llmâ€‘competitorâ€‘analysis

# 2Â Create env & install
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# 3Â Add keys (see .env.example for all options)
$ cp .env.example .env && $EDITOR .env  # paste your secrets

# 4Â Run the showdown
$ python llm_competitor_analysis.py
```

Youâ€™ll get console banners for each vendorâ€™s answer, followed by a tidy leaderboard and a `usage_log.csv` showing you spent pennies.

---

## ğŸ› ï¸ Tech Stack

- **PythonÂ 3.11**  â€¢  **OpenAIÂ Python SDKÂ â‰¥Â 1.23**  â€¢  **AnthropicÂ SDK**  â€¢  `pythonâ€‘dotenv`  â€¢  **Black/Ruff**  â€¢  **pytest**

---

## ğŸ” What to LookÂ for in the Code

* `openai_guard.py`Â â€” modelâ€‘aware param shims, FX conversion, CSV logger.
* `providers.py`Â â€” featherâ€‘weight wrappers with identical signatures.
* `llm_competitor_analysis.py`Â â€” main flow, retry logic, regex JSON rescue.
* `tests/`Â â€” unit tests mocking provider responses and costâ€‘guard maths.

Each commit message follows ConventionalÂ Commits for clean history.



## ğŸ“„ License

Released under the MIT License â€” fork, iterate, and impress.
